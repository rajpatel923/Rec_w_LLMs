{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:41:05.060973Z",
     "start_time": "2024-11-16T23:40:31.809170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"../../Dateset/raw/ratings.json\", lines=True)\n",
    "df.head()\n",
    "df = df.reset_index(drop=True)\n"
   ],
   "id": "5e4d723c9a33c3c",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:45:48.933729Z",
     "start_time": "2024-11-16T23:45:48.929039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieLensDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The movie lens dataset class, this class prepares the dataset for training and validation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        init the dataset object with users, movies and rating data.\n",
    "        :param users: the users id\n",
    "        :param movies: the movies id\n",
    "        :param ratings: the rating data by users on movie \n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the total number of smapels in the dataset\n",
    "        :return: len of dataset\n",
    "        \"\"\"\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Fetches a sample from the dataset.\n",
    "        \"\"\"\n",
    "        row = self.dataframe.iloc[index]\n",
    "        if index >= len(self.dataframe):\n",
    "            raise IndexError(f\"Index {index} out of range for dataset of size {len(self.dataframe)}\")\n",
    "\n",
    "        return {\n",
    "            'user_id': torch.tensor(row['user_id'], dtype=torch.long),\n",
    "            'item_id': torch.tensor(row['item_id'], dtype=torch.long),\n",
    "            'rating': torch.tensor(row['rating'], dtype=torch.float)\n",
    "        }\n",
    "        \n",
    "        "
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:45:50.449546Z",
     "start_time": "2024-11-16T23:45:50.445340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "class RecommendationSystemModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_users,\n",
    "            num_movies,\n",
    "            embedding_size=256,\n",
    "            hidden_size=256,\n",
    "            dropout=0.2,\n",
    "    ):\n",
    "        super(RecommendationSystemModel, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        \n",
    "        self.user_embedding = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_embeddings=self.num_movies, embedding_dim=self.embedding_size)\n",
    "        \n",
    "        self.fc1 =nn.Linear(2*self.embedding_size, self.hidden_size)\n",
    "        self.fc2 = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, users, movies):\n",
    "        user_embedding = self.user_embedding(users)\n",
    "        movie_embedding = self.movie_embedding(movies)\n",
    "        \n",
    "        combined = torch.cat((user_embedding, movie_embedding), 1)\n",
    "        x = self.relu(self.fc1(combined))\n",
    "        x = self.dropout(x)\n",
    "        output = self.fc2(x)\n",
    "        return output\n",
    "       "
   ],
   "id": "f4ee9f637ebfda97",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:45:53.602430Z",
     "start_time": "2024-11-16T23:45:51.223885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le_user = LabelEncoder()\n",
    "le_movie = LabelEncoder()\n",
    "df.user_id = le_user.fit_transform(df['user_id'].values)\n",
    "df.item_id = le_movie.fit_transform(df['item_id'].values)"
   ],
   "id": "50ddf81078e42dc5",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:46:02.443402Z",
     "start_time": "2024-11-16T23:45:56.296302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42,stratify=df.rating.values)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "print(df_train.shape,\"\\n\", df_train)"
   ],
   "id": "cab1df82e448c044",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22792092, 3) \n",
      "           item_id  user_id  rating\n",
      "0             313   110092     2.0\n",
      "1           10713   154036     2.5\n",
      "2             719   116187     3.0\n",
      "3            1069   115011     3.0\n",
      "4           46163   215847     1.5\n",
      "...           ...      ...     ...\n",
      "22792087      345     7197     4.0\n",
      "22792088     3939   200478     4.5\n",
      "22792089     2322    15224     3.5\n",
      "22792090    10267   229139     4.0\n",
      "22792091     6277   142352     4.0\n",
      "\n",
      "[22792092 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:46:08.762018Z",
     "start_time": "2024-11-16T23:46:08.038858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# \n",
    "# train_dataset = MovieLensDataset(df_train)\n",
    "# val_dataset = MovieLensDataset(df_val)\n",
    "# \n",
    "# train_loader = DataLoader(df_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "# val_loader = DataLoader(df_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "# Rename columns if necessary\n",
    "df_train = df_train.rename(columns={\n",
    "    'user_id': 'user_id',\n",
    "    'item_id': 'item_id',\n",
    "    'rating': 'rating'\n",
    "})\n",
    "df_val = df_val.rename(columns={\n",
    "    'user_id': 'user_id',\n",
    "    'item_id': 'item_id',\n",
    "    'rating': 'rating'\n",
    "})\n",
    "\n",
    "# Initialize dataset and DataLoader\n",
    "train_dataset = MovieLensDataset(df_train)\n",
    "val_dataset = MovieLensDataset(df_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "# Debug DataLoader\n",
    "for i, train_data in enumerate(train_loader):\n",
    "    print(train_data)\n",
    "    break\n"
   ],
   "id": "b2e4eb2534e3e80d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': tensor([155629,  55460,  77677, 149120,  23016,  70673,  70737,  33933,  19275,\n",
      "         85593, 204305,  59232,  46208, 146515, 163904,  11374,  86568,  67646,\n",
      "         83555, 191952, 190291,   8702,  90644, 229724, 230021, 116708,  41637,\n",
      "        215599,    671, 218041,  30914, 102355]), 'item_id': tensor([22751,   640, 12504,  4372,  5874,  2652,   257,  3074,  6241,  6443,\n",
      "          315,  1198,   428,  5031,  7979,  1244,  1575,   297,  2312,  1232,\n",
      "         8270,  2429,  3010,  1007, 16404,  7267,  2030, 30726, 18163,  1223,\n",
      "         2639,   843]), 'rating': tensor([4.5000, 4.0000, 4.5000, 4.0000, 3.5000, 3.0000, 5.0000, 5.0000, 3.5000,\n",
      "        0.5000, 5.0000, 5.0000, 0.5000, 3.0000, 3.0000, 2.0000, 2.0000, 3.0000,\n",
      "        3.0000, 4.5000, 4.0000, 2.0000, 2.0000, 4.0000, 2.5000, 4.5000, 4.0000,\n",
      "        5.0000, 4.0000, 4.5000, 3.0000, 4.0000])}\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:46:16.352045Z",
     "start_time": "2024-11-16T23:46:16.348198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ],
   "id": "39133bfecc71fde4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T02:07:35.906471Z",
     "start_time": "2024-11-17T00:22:45.525450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "recommendation_model = RecommendationSystemModel(\n",
    "    num_users=len(le_user.classes_),\n",
    "    num_movies=len(le_movie.classes_),\n",
    "    embedding_size=128,\n",
    "    hidden_size=256,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    recommendation_model.parameters(),\n",
    "    lr=1e-3,\n",
    ")\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "def log_progress(epoch, step, total_loss, log_progress_step, data_size, losses):\n",
    "    avg_loss = total_loss / log_progress_step\n",
    "    sys.stderr.write(\n",
    "        f\"\\r{epoch+1:02d}/{EPOCHS:02d}  | step: {step}/{data_size} | avg_loss: {avg_loss:.4f}\"\n",
    "    )\n",
    "    sys.stderr.flush()\n",
    "    losses.append(avg_loss)\n",
    "total_loss =0\n",
    "log_progress_step = 100\n",
    "losses = []\n",
    "train_dataset_size = len(df_train)\n",
    "print(f\"traning on {train_dataset_size} samples\")\n",
    "\n",
    "recommendation_model.train()\n",
    "for e in range(EPOCHS):\n",
    "    step_count = 0\n",
    "    for i, train_data in enumerate(train_loader):\n",
    "        outputs = recommendation_model(train_data['user_id'].to(device), train_data['item_id'].to(device))\n",
    "        outputs = outputs.squeeze()\n",
    "        ratings = (\n",
    "            train_data[\"rating\"].to(torch.float32).to(device)\n",
    "        )\n",
    "        loss = loss_fn(outputs,ratings)\n",
    "        total_loss+=loss.sum().item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step_count+=len(train_data['user_id'])\n",
    "        \n",
    "        if(\n",
    "            step_count%log_progress_step==0 or i == len(train_loader)-1\n",
    "        ):\n",
    "            log_progress(\n",
    "                e, step_count, total_loss,log_progress_step,train_dataset_size, losses\n",
    "            )\n",
    "            total_loss = 0"
   ],
   "id": "2c0a3e4f82863c4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traning on 22792092 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/02  | step: 7711200/22792092 | avg_loss: 0.2111"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 35\u001B[0m\n\u001B[1;32m     33\u001B[0m step_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, train_data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m---> 35\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m recommendation_model(\u001B[43mtrain_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43muser_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m, train_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitem_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m     36\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39msqueeze()\n\u001B[1;32m     37\u001B[0m     ratings \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     38\u001B[0m         train_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrating\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     39\u001B[0m     )\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T23:36:15.279159Z",
     "start_time": "2024-11-16T23:36:06.070170Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1a8a47def2153ebb",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 2602010\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 2602010\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_d\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m   \u001B[49m\u001B[43mtrain_d\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1465\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1463\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1464\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[0;32m-> 1465\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1491\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1489\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[1;32m   1490\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[0;32m-> 1491\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1492\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/_utils.py:715\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    711\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    712\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[1;32m    714\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 715\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[0;31mKeyError\u001B[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 2602010\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/envs/Movie_rec/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n    raise KeyError(key) from err\nKeyError: 2602010\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "80b638d45a4a9609"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
